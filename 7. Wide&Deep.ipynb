{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Wide&Deep(더 넓고, 더 길게)\n",
    "***\n",
    "\n",
    "- Wide(더 넓게): 더 많은 input($x$값)을 통해 output($y$) 예측\n",
    "- Deep(더 깊게): 더 많은 선형 레이어(Linear Layer), 활성화 함수(Activation Function) 통해 output($y$) 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Wide(더 넓게)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wide가 필요한 이유: Output 예측의 정확성과 신뢰성을 높이기 위해 더 많은 변수들(독립 변수, $x$값,input)을 고려할 필요가 있음  \n",
    "- 행렬 곱(Matrix Multiplication): $XW = \\hat{Y}$\n",
    "```python\n",
    "x_data = [[2.1, 0.1], # 4X2 Input 행렬\n",
    "          [4.2, 0.8],\n",
    "          [3.1, 0.9],\n",
    "          [3.3, 0.2]]\n",
    "y_data = [[0.0],      # 2X1 Output 행렬\n",
    "          [1.0],\n",
    "          [0.0],\n",
    "          [1.0]]\n",
    "linear = torch.nn.Linear(2,1) # Input 2개로 output 1개 예측\n",
    "y_pred = linear(x_data)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Deep(더 깊게)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep이 필요한 이유: Output 예측의 정확성과 신뢰성을 높이기 위해 더 많은 레이어들을 넣을 필요가 있음\n",
    "- 선형 레이어(Linear Layer), 활성화 레이어(Activation Layer) 다수 추가 및 종류 다양화\n",
    "\n",
    "```python\n",
    "sigmoid = torch.nnSigmoid()\n",
    "l1 = torch.nn.Linear(2, 4)\n",
    "l2 = torch.nn.Linear(4, 3)\n",
    "l3 = torch.nn.Linear(3, 1) # 3개의 레이어(행렬곱 개념)\n",
    "out1 = sigmoid(l1(x_data))\n",
    "out2 = sigmoid(l2(out1))\n",
    "y_pred = sigmoid(l3(out2)) # 활성화 레이어로 연결\n",
    "```\n",
    "\n",
    "- Sigmoid: Vanishing Gradient Problem(경사도 지움 문제)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Code Practice: Wide&Deep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. 기초 데이터 가져오기(Classifying Diabetes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([759, 8])\n",
      "torch.Size([759, 1])\n",
      "tensor([[-0.2941,  0.4874,  0.1803, -0.2929,  0.0000,  0.0015, -0.5312, -0.0333],\n",
      "        [-0.8824, -0.1457,  0.0820, -0.4141,  0.0000, -0.2072, -0.7669, -0.6667],\n",
      "        [-0.0588,  0.8392,  0.0492,  0.0000,  0.0000, -0.3055, -0.4927, -0.6333],\n",
      "        [-0.8824, -0.1055,  0.0820, -0.5354, -0.7778, -0.1624, -0.9240,  0.0000],\n",
      "        [ 0.0000,  0.3769, -0.3443, -0.2929, -0.6028,  0.2846,  0.8873, -0.6000],\n",
      "        [-0.4118,  0.1658,  0.2131,  0.0000,  0.0000, -0.2370, -0.8950, -0.7000],\n",
      "        [-0.6471, -0.2161, -0.1803, -0.3535, -0.7920, -0.0760, -0.8548, -0.8333],\n",
      "        [ 0.1765,  0.1558,  0.0000,  0.0000,  0.0000,  0.0522, -0.9522, -0.7333],\n",
      "        [-0.7647,  0.9799,  0.1475, -0.0909,  0.2837, -0.0909, -0.9317,  0.0667],\n",
      "        [-0.0588,  0.2563,  0.5738,  0.0000,  0.0000,  0.0000, -0.8685,  0.1000]])\n"
     ]
    }
   ],
   "source": [
    "# Step 0 : Import and define our data\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "\n",
    "xy = np.loadtxt('diabetes.csv', delimiter=',', dtype=np.float32)\n",
    "x_data = Variable(torch.from_numpy(xy[:, 0:-1]))\n",
    "y_data = Variable(torch.from_numpy(xy[:, [-1]]))\n",
    "\n",
    "print(x_data.data.shape) # torch.Size([759, 8])\n",
    "print(y_data.data.shape) # torch.Size([759, 1])\n",
    "\n",
    "print(x_data[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 모델 클래스 생성(Model Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 : Deesign our model\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate 2 nn.linear module\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        self.l1 = torch.nn.Linear(8, 6) # Wide: 8 Inputs\n",
    "        self.l2 = torch.nn.Linear(6, 4)\n",
    "        self.l3 = torch.nn.Linear(4, 1) # Deep: 3 layers\n",
    "\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Variable of input data and we must return a Variable of output data. We can use Modules defined in the constructor as well as arbitrary operators on Varaibles.\n",
    "        \"\"\"\n",
    "        out1 = self.sigmoid(self.l1(x))\n",
    "        out2 = self.sigmoid(self.l2(out1))\n",
    "        y_pred = self.sigmoid(self.l3(out2)) # Deep: 3 more layers(Activation Layer)\n",
    "        return y_pred\n",
    "\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 손실 함수 및 최적화 함수 구성(Loss & Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dbtmd\\anaconda3\\envs\\py37_64\\lib\\site-packages\\torch\\nn\\_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "# Step 2 : Construct loss and optimizer\n",
    "\n",
    "\"\"\"\n",
    "The call to model.parameters() in the SGD constructor will contain the learnable parameters of the 2 nn.linear modules members of the model.\n",
    "\"\"\"\n",
    "criterion = torch.nn.BCELoss(size_average=True)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 훈련 순환 돌리기(Training Cycle: Forward, Backward, Update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.6961)\n",
      "1 tensor(0.6912)\n",
      "2 tensor(0.6869)\n",
      "3 tensor(0.6829)\n",
      "4 tensor(0.6793)\n",
      "5 tensor(0.6761)\n",
      "6 tensor(0.6732)\n",
      "7 tensor(0.6706)\n",
      "8 tensor(0.6682)\n",
      "9 tensor(0.6660)\n",
      "10 tensor(0.6641)\n",
      "11 tensor(0.6623)\n",
      "12 tensor(0.6607)\n",
      "13 tensor(0.6593)\n",
      "14 tensor(0.6580)\n",
      "15 tensor(0.6568)\n",
      "16 tensor(0.6557)\n",
      "17 tensor(0.6547)\n",
      "18 tensor(0.6538)\n",
      "19 tensor(0.6530)\n",
      "20 tensor(0.6523)\n",
      "21 tensor(0.6516)\n",
      "22 tensor(0.6510)\n",
      "23 tensor(0.6505)\n",
      "24 tensor(0.6500)\n",
      "25 tensor(0.6496)\n",
      "26 tensor(0.6491)\n",
      "27 tensor(0.6488)\n",
      "28 tensor(0.6484)\n",
      "29 tensor(0.6481)\n",
      "30 tensor(0.6478)\n",
      "31 tensor(0.6476)\n",
      "32 tensor(0.6474)\n",
      "33 tensor(0.6471)\n",
      "34 tensor(0.6469)\n",
      "35 tensor(0.6468)\n",
      "36 tensor(0.6466)\n",
      "37 tensor(0.6465)\n",
      "38 tensor(0.6463)\n",
      "39 tensor(0.6462)\n",
      "40 tensor(0.6461)\n",
      "41 tensor(0.6460)\n",
      "42 tensor(0.6459)\n",
      "43 tensor(0.6458)\n",
      "44 tensor(0.6458)\n",
      "45 tensor(0.6457)\n",
      "46 tensor(0.6456)\n",
      "47 tensor(0.6456)\n",
      "48 tensor(0.6455)\n",
      "49 tensor(0.6455)\n",
      "50 tensor(0.6454)\n",
      "51 tensor(0.6454)\n",
      "52 tensor(0.6453)\n",
      "53 tensor(0.6453)\n",
      "54 tensor(0.6453)\n",
      "55 tensor(0.6452)\n",
      "56 tensor(0.6452)\n",
      "57 tensor(0.6452)\n",
      "58 tensor(0.6452)\n",
      "59 tensor(0.6452)\n",
      "60 tensor(0.6451)\n",
      "61 tensor(0.6451)\n",
      "62 tensor(0.6451)\n",
      "63 tensor(0.6451)\n",
      "64 tensor(0.6451)\n",
      "65 tensor(0.6451)\n",
      "66 tensor(0.6451)\n",
      "67 tensor(0.6451)\n",
      "68 tensor(0.6450)\n",
      "69 tensor(0.6450)\n",
      "70 tensor(0.6450)\n",
      "71 tensor(0.6450)\n",
      "72 tensor(0.6450)\n",
      "73 tensor(0.6450)\n",
      "74 tensor(0.6450)\n",
      "75 tensor(0.6450)\n",
      "76 tensor(0.6450)\n",
      "77 tensor(0.6450)\n",
      "78 tensor(0.6450)\n",
      "79 tensor(0.6450)\n",
      "80 tensor(0.6450)\n",
      "81 tensor(0.6450)\n",
      "82 tensor(0.6450)\n",
      "83 tensor(0.6450)\n",
      "84 tensor(0.6450)\n",
      "85 tensor(0.6450)\n",
      "86 tensor(0.6450)\n",
      "87 tensor(0.6450)\n",
      "88 tensor(0.6450)\n",
      "89 tensor(0.6450)\n",
      "90 tensor(0.6450)\n",
      "91 tensor(0.6450)\n",
      "92 tensor(0.6450)\n",
      "93 tensor(0.6450)\n",
      "94 tensor(0.6450)\n",
      "95 tensor(0.6450)\n",
      "96 tensor(0.6450)\n",
      "97 tensor(0.6450)\n",
      "98 tensor(0.6450)\n",
      "99 tensor(0.6450)\n"
     ]
    }
   ],
   "source": [
    "# Step 3 : Training loop\n",
    "\n",
    "for epoch in range(100):\n",
    "    # Forward pass : Compute predicted y by passing x to the model\n",
    "    y_pred = model(x_data)\n",
    "    \n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    print(epoch, loss.data)\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.4 Exercise: Classifying Diabetes with deep nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.7142)\n",
      "1 tensor(0.7106)\n",
      "2 tensor(0.7072)\n",
      "3 tensor(0.7039)\n",
      "4 tensor(0.7009)\n",
      "5 tensor(0.6980)\n",
      "6 tensor(0.6953)\n",
      "7 tensor(0.6927)\n",
      "8 tensor(0.6902)\n",
      "9 tensor(0.6879)\n",
      "10 tensor(0.6858)\n",
      "11 tensor(0.6837)\n",
      "12 tensor(0.6817)\n",
      "13 tensor(0.6799)\n",
      "14 tensor(0.6781)\n",
      "15 tensor(0.6765)\n",
      "16 tensor(0.6749)\n",
      "17 tensor(0.6734)\n",
      "18 tensor(0.6720)\n",
      "19 tensor(0.6707)\n",
      "20 tensor(0.6694)\n",
      "21 tensor(0.6682)\n",
      "22 tensor(0.6670)\n",
      "23 tensor(0.6660)\n",
      "24 tensor(0.6649)\n",
      "25 tensor(0.6640)\n",
      "26 tensor(0.6630)\n",
      "27 tensor(0.6622)\n",
      "28 tensor(0.6613)\n",
      "29 tensor(0.6605)\n",
      "30 tensor(0.6598)\n",
      "31 tensor(0.6591)\n",
      "32 tensor(0.6584)\n",
      "33 tensor(0.6577)\n",
      "34 tensor(0.6571)\n",
      "35 tensor(0.6566)\n",
      "36 tensor(0.6560)\n",
      "37 tensor(0.6555)\n",
      "38 tensor(0.6550)\n",
      "39 tensor(0.6545)\n",
      "40 tensor(0.6541)\n",
      "41 tensor(0.6536)\n",
      "42 tensor(0.6532)\n",
      "43 tensor(0.6528)\n",
      "44 tensor(0.6525)\n",
      "45 tensor(0.6521)\n",
      "46 tensor(0.6518)\n",
      "47 tensor(0.6515)\n",
      "48 tensor(0.6512)\n",
      "49 tensor(0.6509)\n",
      "50 tensor(0.6506)\n",
      "51 tensor(0.6504)\n",
      "52 tensor(0.6501)\n",
      "53 tensor(0.6499)\n",
      "54 tensor(0.6497)\n",
      "55 tensor(0.6495)\n",
      "56 tensor(0.6493)\n",
      "57 tensor(0.6491)\n",
      "58 tensor(0.6489)\n",
      "59 tensor(0.6487)\n",
      "60 tensor(0.6486)\n",
      "61 tensor(0.6484)\n",
      "62 tensor(0.6482)\n",
      "63 tensor(0.6481)\n",
      "64 tensor(0.6480)\n",
      "65 tensor(0.6478)\n",
      "66 tensor(0.6477)\n",
      "67 tensor(0.6476)\n",
      "68 tensor(0.6475)\n",
      "69 tensor(0.6474)\n",
      "70 tensor(0.6473)\n",
      "71 tensor(0.6472)\n",
      "72 tensor(0.6471)\n",
      "73 tensor(0.6470)\n",
      "74 tensor(0.6469)\n",
      "75 tensor(0.6469)\n",
      "76 tensor(0.6468)\n",
      "77 tensor(0.6467)\n",
      "78 tensor(0.6466)\n",
      "79 tensor(0.6466)\n",
      "80 tensor(0.6465)\n",
      "81 tensor(0.6465)\n",
      "82 tensor(0.6464)\n",
      "83 tensor(0.6464)\n",
      "84 tensor(0.6463)\n",
      "85 tensor(0.6463)\n",
      "86 tensor(0.6462)\n",
      "87 tensor(0.6462)\n",
      "88 tensor(0.6461)\n",
      "89 tensor(0.6461)\n",
      "90 tensor(0.6461)\n",
      "91 tensor(0.6460)\n",
      "92 tensor(0.6460)\n",
      "93 tensor(0.6459)\n",
      "94 tensor(0.6459)\n",
      "95 tensor(0.6459)\n",
      "96 tensor(0.6459)\n",
      "97 tensor(0.6458)\n",
      "98 tensor(0.6458)\n",
      "99 tensor(0.6458)\n"
     ]
    }
   ],
   "source": [
    "# Step 1 : Deesign our model\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate 2 nn.linear module\n",
    "        \"\"\"\n",
    "        super(Model, self).__init__()\n",
    "        self.l1 = torch.nn.Linear(8, 7) # Wide: 8 Inputs\n",
    "        self.l2 = torch.nn.Linear(7, 6)\n",
    "        self.l3 = torch.nn.Linear(6, 5)\n",
    "        self.l4 = torch.nn.Linear(5, 4)\n",
    "        self.l5 = torch.nn.Linear(4, 3) \n",
    "        self.l6 = torch.nn.Linear(3, 2)\n",
    "        self.l7 = torch.nn.Linear(2, 1) # Deep: 7 layers\n",
    "\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.tanh = torch.nn.Tanh() # 새로운 활성화 함수들 추가\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Variable of input data and we must return a Variable of output data. We can use Modules defined in the constructor as well as arbitrary operators on Varaibles.\n",
    "        \"\"\"\n",
    "        out1 = self.sigmoid(self.l1(x))\n",
    "        out2 = self.relu(self.l2(out1))\n",
    "        out3 = self.relu(self.l3(out2))\n",
    "        out4 = self.tanh(self.l4(out3))\n",
    "        out5 = self.sigmoid(self.l5(out4))\n",
    "        out6 = self.relu(self.l6(out5))\n",
    "        y_pred = self.sigmoid(self.l7(out6)) # Deep: 7 more layers(Activation Layer)\n",
    "        return y_pred\n",
    "\n",
    "model = Model()\n",
    "\n",
    "# Step 2 : Construct loss and optimizer\n",
    "\n",
    "\"\"\"\n",
    "The call to model.parameters() in the SGD constructor will contain the learnable parameters of the 2 nn.linear modules members of the model.\n",
    "\"\"\"\n",
    "criterion = torch.nn.BCELoss(size_average=True)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Step 3 : Training loop\n",
    "\n",
    "for epoch in range(100):\n",
    "    # Forward pass : Compute predicted y by passing x to the model\n",
    "    y_pred = model(x_data)\n",
    "    \n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, y_data)\n",
    "    print(epoch, loss.data)\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_64",
   "language": "python",
   "name": "py37_64"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
