{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Back-Propagation & Autograd(역전파 & 미분 자동화)\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Computational Graph & Chain Rule(연산 그래프와 연쇄 법칙)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Computatonal Graph(연산 그래프)\n",
    "- 간단한 네트워크에선 경사(gradient)를 구하기 쉬움\n",
    "    - 단순히 미분하면 됨\n",
    "- 하지만 복잡한 네트워크라면? \n",
    "    - 미분하기가 어려움\n",
    " ![4_BP](slides/이미지/4_BP.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Chain Rule(연쇄 법칙)\n",
    "-  Chain Rule(연쇄법칙): 합성함수의 미분법 \n",
    "    - 합성함수의 미분은 합성함수를 구성하는 각 함수의 미분의 곱으로 나타낼 수 있음\n",
    "    - $f=f(g)$ and $g=g(x)$, ${\\partial f \\over \\partial x} = {\\partial f \\over \\partial g}*{\\partial g \\over \\partial x}$\n",
    "- Chain Rule을 이용하면 복잡한 네트워크의 경사(Gradient)를 구할 수 있음 \n",
    "    - 각 과정들의 편미분값을 구한후, 해당 값들을 모두 곱해주면 네트워크의 경사(Gradient)가 나옴\n",
    "#### <순서>\n",
    "\n",
    "    1. Local Gradient 구하기\n",
    "    2. 주어진 Global Gradient 값과 Local Gradient 곱하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3 Backward Propagation(역전파)\n",
    "1. Forward pass(전파) 실시\n",
    "    - 변수($x$값, $y$값), 가중치($w$값)을 모델에 input값으로 넣어 가동\n",
    "    - 손실(loss) 계산\n",
    "2. Backward Propagation(역전파) 실시\n",
    "    - 각각 Gate(node, neuron)의 local gradient 계산(뒤에서부터)\n",
    "    - 계산시 연쇄 법칙(Chain Rule) 사용\n",
    "    - 최종 모델의 경사(Gradient) 계산 ($\\partial loss \\over \\partial w$)\n",
    "3. 가중치 업데이트 실시\n",
    "    - 계산된 모델의 경사를 가중치에 적용하여 가중치를 조정\n",
    "    - 조정된 가중치로 모델 훈련 재가동"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Code Practice: Backward Propagation & Autograd(역전파 & 미분 자동화)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 파이토치에서, 역전파를 진행하기 위해선 변수들만 설정해주면 된다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 기본 데이터 생성 & 가중치 텐서 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable # Variable: 파이토치에서 역전파시 사용하는 함수\n",
    "\n",
    "x_data = [1.0, 2.0, 3.0]\n",
    "y_data = [2.0, 4.0, 6.0]\n",
    "\n",
    "w = Variable(torch.Tensor([1.0]), requires_grad = True) # Any random value\n",
    "\n",
    "# 1. 텐서 생성 2. Variable 함수 사용 3. 경사(gradient) 계산 필요 여부 설정 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 모델 및 손실 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict (before training) 4 tensor(4.)\n"
     ]
    }
   ],
   "source": [
    "# 1. our forward pass model \n",
    "def forward(x):\n",
    "    return x * w\n",
    "\n",
    "# 2. Loss function\n",
    "def loss(x, y):\n",
    "    y_pred = forward(x)\n",
    "    return (y_pred - y) * (y_pred - y)\n",
    "\n",
    "# Before training\n",
    "print(\"predict (before training)\", 4, forward(4).data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 훈련: 전파, 역전파 그리고 가중치 업데이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tgrad:  1.0 2.0 tensor(-2.)\n",
      "\tgrad:  2.0 4.0 tensor(-7.8400)\n",
      "\tgrad:  3.0 6.0 tensor(-16.2288)\n",
      "progress: 0 w= tensor([1.2607], requires_grad=True) loss= tensor(7.3159)\n",
      "\tgrad:  1.0 2.0 tensor(-1.4786)\n",
      "\tgrad:  2.0 4.0 tensor(-5.7962)\n",
      "\tgrad:  3.0 6.0 tensor(-11.9981)\n",
      "progress: 1 w= tensor([1.4534], requires_grad=True) loss= tensor(3.9988)\n",
      "\tgrad:  1.0 2.0 tensor(-1.0932)\n",
      "\tgrad:  2.0 4.0 tensor(-4.2852)\n",
      "\tgrad:  3.0 6.0 tensor(-8.8704)\n",
      "progress: 2 w= tensor([1.5959], requires_grad=True) loss= tensor(2.1857)\n",
      "\tgrad:  1.0 2.0 tensor(-0.8082)\n",
      "\tgrad:  2.0 4.0 tensor(-3.1681)\n",
      "\tgrad:  3.0 6.0 tensor(-6.5580)\n",
      "progress: 3 w= tensor([1.7012], requires_grad=True) loss= tensor(1.1946)\n",
      "\tgrad:  1.0 2.0 tensor(-0.5975)\n",
      "\tgrad:  2.0 4.0 tensor(-2.3422)\n",
      "\tgrad:  3.0 6.0 tensor(-4.8484)\n",
      "progress: 4 w= tensor([1.7791], requires_grad=True) loss= tensor(0.6530)\n",
      "\tgrad:  1.0 2.0 tensor(-0.4417)\n",
      "\tgrad:  2.0 4.0 tensor(-1.7316)\n",
      "\tgrad:  3.0 6.0 tensor(-3.5845)\n",
      "progress: 5 w= tensor([1.8367], requires_grad=True) loss= tensor(0.3569)\n",
      "\tgrad:  1.0 2.0 tensor(-0.3266)\n",
      "\tgrad:  2.0 4.0 tensor(-1.2802)\n",
      "\tgrad:  3.0 6.0 tensor(-2.6500)\n",
      "progress: 6 w= tensor([1.8793], requires_grad=True) loss= tensor(0.1951)\n",
      "\tgrad:  1.0 2.0 tensor(-0.2414)\n",
      "\tgrad:  2.0 4.0 tensor(-0.9465)\n",
      "\tgrad:  3.0 6.0 tensor(-1.9592)\n",
      "progress: 7 w= tensor([1.9107], requires_grad=True) loss= tensor(0.1066)\n",
      "\tgrad:  1.0 2.0 tensor(-0.1785)\n",
      "\tgrad:  2.0 4.0 tensor(-0.6997)\n",
      "\tgrad:  3.0 6.0 tensor(-1.4485)\n",
      "progress: 8 w= tensor([1.9340], requires_grad=True) loss= tensor(0.0583)\n",
      "\tgrad:  1.0 2.0 tensor(-0.1320)\n",
      "\tgrad:  2.0 4.0 tensor(-0.5173)\n",
      "\tgrad:  3.0 6.0 tensor(-1.0709)\n",
      "progress: 9 w= tensor([1.9512], requires_grad=True) loss= tensor(0.0319)\n",
      "\tgrad:  1.0 2.0 tensor(-0.0976)\n",
      "\tgrad:  2.0 4.0 tensor(-0.3825)\n",
      "\tgrad:  3.0 6.0 tensor(-0.7917)\n",
      "progress: 10 w= tensor([1.9639], requires_grad=True) loss= tensor(0.0174)\n",
      "\tgrad:  1.0 2.0 tensor(-0.0721)\n",
      "\tgrad:  2.0 4.0 tensor(-0.2828)\n",
      "\tgrad:  3.0 6.0 tensor(-0.5853)\n",
      "progress: 11 w= tensor([1.9733], requires_grad=True) loss= tensor(0.0095)\n",
      "\tgrad:  1.0 2.0 tensor(-0.0533)\n",
      "\tgrad:  2.0 4.0 tensor(-0.2090)\n",
      "\tgrad:  3.0 6.0 tensor(-0.4327)\n",
      "progress: 12 w= tensor([1.9803], requires_grad=True) loss= tensor(0.0052)\n",
      "\tgrad:  1.0 2.0 tensor(-0.0394)\n",
      "\tgrad:  2.0 4.0 tensor(-0.1546)\n",
      "\tgrad:  3.0 6.0 tensor(-0.3199)\n",
      "progress: 13 w= tensor([1.9854], requires_grad=True) loss= tensor(0.0028)\n",
      "\tgrad:  1.0 2.0 tensor(-0.0291)\n",
      "\tgrad:  2.0 4.0 tensor(-0.1143)\n",
      "\tgrad:  3.0 6.0 tensor(-0.2365)\n",
      "progress: 14 w= tensor([1.9892], requires_grad=True) loss= tensor(0.0016)\n",
      "\tgrad:  1.0 2.0 tensor(-0.0215)\n",
      "\tgrad:  2.0 4.0 tensor(-0.0845)\n",
      "\tgrad:  3.0 6.0 tensor(-0.1749)\n",
      "progress: 15 w= tensor([1.9920], requires_grad=True) loss= tensor(0.0008)\n",
      "\tgrad:  1.0 2.0 tensor(-0.0159)\n",
      "\tgrad:  2.0 4.0 tensor(-0.0625)\n",
      "\tgrad:  3.0 6.0 tensor(-0.1293)\n",
      "progress: 16 w= tensor([1.9941], requires_grad=True) loss= tensor(0.0005)\n",
      "\tgrad:  1.0 2.0 tensor(-0.0118)\n",
      "\tgrad:  2.0 4.0 tensor(-0.0462)\n",
      "\tgrad:  3.0 6.0 tensor(-0.0956)\n",
      "progress: 17 w= tensor([1.9956], requires_grad=True) loss= tensor(0.0003)\n",
      "\tgrad:  1.0 2.0 tensor(-0.0087)\n",
      "\tgrad:  2.0 4.0 tensor(-0.0341)\n",
      "\tgrad:  3.0 6.0 tensor(-0.0707)\n",
      "progress: 18 w= tensor([1.9968], requires_grad=True) loss= tensor(0.0001)\n",
      "\tgrad:  1.0 2.0 tensor(-0.0064)\n",
      "\tgrad:  2.0 4.0 tensor(-0.0252)\n",
      "\tgrad:  3.0 6.0 tensor(-0.0522)\n",
      "progress: 19 w= tensor([1.9976], requires_grad=True) loss= tensor(7.5804e-05)\n",
      "\tgrad:  1.0 2.0 tensor(-0.0048)\n",
      "\tgrad:  2.0 4.0 tensor(-0.0187)\n",
      "\tgrad:  3.0 6.0 tensor(-0.0386)\n",
      "progress: 20 w= tensor([1.9982], requires_grad=True) loss= tensor(4.1433e-05)\n",
      "\tgrad:  1.0 2.0 tensor(-0.0035)\n",
      "\tgrad:  2.0 4.0 tensor(-0.0138)\n",
      "\tgrad:  3.0 6.0 tensor(-0.0286)\n",
      "progress: 21 w= tensor([1.9987], requires_grad=True) loss= tensor(2.2647e-05)\n",
      "\tgrad:  1.0 2.0 tensor(-0.0026)\n",
      "\tgrad:  2.0 4.0 tensor(-0.0102)\n",
      "\tgrad:  3.0 6.0 tensor(-0.0211)\n",
      "progress: 22 w= tensor([1.9990], requires_grad=True) loss= tensor(1.2377e-05)\n",
      "\tgrad:  1.0 2.0 tensor(-0.0019)\n",
      "\tgrad:  2.0 4.0 tensor(-0.0075)\n",
      "\tgrad:  3.0 6.0 tensor(-0.0156)\n",
      "progress: 23 w= tensor([1.9993], requires_grad=True) loss= tensor(6.7684e-06)\n",
      "\tgrad:  1.0 2.0 tensor(-0.0014)\n",
      "\tgrad:  2.0 4.0 tensor(-0.0056)\n",
      "\tgrad:  3.0 6.0 tensor(-0.0115)\n",
      "progress: 24 w= tensor([1.9995], requires_grad=True) loss= tensor(3.7001e-06)\n",
      "\tgrad:  1.0 2.0 tensor(-0.0011)\n",
      "\tgrad:  2.0 4.0 tensor(-0.0041)\n",
      "\tgrad:  3.0 6.0 tensor(-0.0085)\n",
      "progress: 25 w= tensor([1.9996], requires_grad=True) loss= tensor(2.0219e-06)\n",
      "\tgrad:  1.0 2.0 tensor(-0.0008)\n",
      "\tgrad:  2.0 4.0 tensor(-0.0030)\n",
      "\tgrad:  3.0 6.0 tensor(-0.0063)\n",
      "progress: 26 w= tensor([1.9997], requires_grad=True) loss= tensor(1.1045e-06)\n",
      "\tgrad:  1.0 2.0 tensor(-0.0006)\n",
      "\tgrad:  2.0 4.0 tensor(-0.0023)\n",
      "\tgrad:  3.0 6.0 tensor(-0.0047)\n",
      "progress: 27 w= tensor([1.9998], requires_grad=True) loss= tensor(6.0411e-07)\n",
      "\tgrad:  1.0 2.0 tensor(-0.0004)\n",
      "\tgrad:  2.0 4.0 tensor(-0.0017)\n",
      "\tgrad:  3.0 6.0 tensor(-0.0034)\n",
      "progress: 28 w= tensor([1.9998], requires_grad=True) loss= tensor(3.2960e-07)\n",
      "\tgrad:  1.0 2.0 tensor(-0.0003)\n",
      "\tgrad:  2.0 4.0 tensor(-0.0012)\n",
      "\tgrad:  3.0 6.0 tensor(-0.0025)\n",
      "progress: 29 w= tensor([1.9999], requires_grad=True) loss= tensor(1.8051e-07)\n",
      "\tgrad:  1.0 2.0 tensor(-0.0002)\n",
      "\tgrad:  2.0 4.0 tensor(-0.0009)\n",
      "\tgrad:  3.0 6.0 tensor(-0.0019)\n",
      "progress: 30 w= tensor([1.9999], requires_grad=True) loss= tensor(9.8744e-08)\n",
      "\tgrad:  1.0 2.0 tensor(-0.0002)\n",
      "\tgrad:  2.0 4.0 tensor(-0.0007)\n",
      "\tgrad:  3.0 6.0 tensor(-0.0014)\n",
      "progress: 31 w= tensor([1.9999], requires_grad=True) loss= tensor(5.4148e-08)\n",
      "\tgrad:  1.0 2.0 tensor(-0.0001)\n",
      "\tgrad:  2.0 4.0 tensor(-0.0005)\n",
      "\tgrad:  3.0 6.0 tensor(-0.0010)\n",
      "progress: 32 w= tensor([2.0000], requires_grad=True) loss= tensor(2.9468e-08)\n",
      "\tgrad:  1.0 2.0 tensor(-9.3937e-05)\n",
      "\tgrad:  2.0 4.0 tensor(-0.0004)\n",
      "\tgrad:  3.0 6.0 tensor(-0.0008)\n",
      "progress: 33 w= tensor([2.0000], requires_grad=True) loss= tensor(1.6088e-08)\n",
      "\tgrad:  1.0 2.0 tensor(-6.9380e-05)\n",
      "\tgrad:  2.0 4.0 tensor(-0.0003)\n",
      "\tgrad:  3.0 6.0 tensor(-0.0006)\n",
      "progress: 34 w= tensor([2.0000], requires_grad=True) loss= tensor(8.7348e-09)\n",
      "\tgrad:  1.0 2.0 tensor(-5.1260e-05)\n",
      "\tgrad:  2.0 4.0 tensor(-0.0002)\n",
      "\tgrad:  3.0 6.0 tensor(-0.0004)\n",
      "progress: 35 w= tensor([2.0000], requires_grad=True) loss= tensor(4.8467e-09)\n",
      "\tgrad:  1.0 2.0 tensor(-3.7909e-05)\n",
      "\tgrad:  2.0 4.0 tensor(-0.0001)\n",
      "\tgrad:  3.0 6.0 tensor(-0.0003)\n",
      "progress: 36 w= tensor([2.0000], requires_grad=True) loss= tensor(2.6521e-09)\n",
      "\tgrad:  1.0 2.0 tensor(-2.8133e-05)\n",
      "\tgrad:  2.0 4.0 tensor(-0.0001)\n",
      "\tgrad:  3.0 6.0 tensor(-0.0002)\n",
      "progress: 37 w= tensor([2.0000], requires_grad=True) loss= tensor(1.4552e-09)\n",
      "\tgrad:  1.0 2.0 tensor(-2.0981e-05)\n",
      "\tgrad:  2.0 4.0 tensor(-8.2016e-05)\n",
      "\tgrad:  3.0 6.0 tensor(-0.0002)\n",
      "progress: 38 w= tensor([2.0000], requires_grad=True) loss= tensor(7.9149e-10)\n",
      "\tgrad:  1.0 2.0 tensor(-1.5497e-05)\n",
      "\tgrad:  2.0 4.0 tensor(-6.1035e-05)\n",
      "\tgrad:  3.0 6.0 tensor(-0.0001)\n",
      "progress: 39 w= tensor([2.0000], requires_grad=True) loss= tensor(4.4020e-10)\n",
      "\tgrad:  1.0 2.0 tensor(-1.1444e-05)\n",
      "\tgrad:  2.0 4.0 tensor(-4.4823e-05)\n",
      "\tgrad:  3.0 6.0 tensor(-9.1553e-05)\n",
      "progress: 40 w= tensor([2.0000], requires_grad=True) loss= tensor(2.3283e-10)\n",
      "\tgrad:  1.0 2.0 tensor(-8.3447e-06)\n",
      "\tgrad:  2.0 4.0 tensor(-3.2425e-05)\n",
      "\tgrad:  3.0 6.0 tensor(-6.5804e-05)\n",
      "progress: 41 w= tensor([2.0000], requires_grad=True) loss= tensor(1.2028e-10)\n",
      "\tgrad:  1.0 2.0 tensor(-5.9605e-06)\n",
      "\tgrad:  2.0 4.0 tensor(-2.2888e-05)\n",
      "\tgrad:  3.0 6.0 tensor(-4.5776e-05)\n",
      "progress: 42 w= tensor([2.0000], requires_grad=True) loss= tensor(5.8208e-11)\n",
      "\tgrad:  1.0 2.0 tensor(-4.2915e-06)\n",
      "\tgrad:  2.0 4.0 tensor(-1.7166e-05)\n",
      "\tgrad:  3.0 6.0 tensor(-3.7193e-05)\n",
      "progress: 43 w= tensor([2.0000], requires_grad=True) loss= tensor(3.8426e-11)\n",
      "\tgrad:  1.0 2.0 tensor(-3.3379e-06)\n",
      "\tgrad:  2.0 4.0 tensor(-1.3351e-05)\n",
      "\tgrad:  3.0 6.0 tensor(-2.8610e-05)\n",
      "progress: 44 w= tensor([2.0000], requires_grad=True) loss= tensor(2.2737e-11)\n",
      "\tgrad:  1.0 2.0 tensor(-2.6226e-06)\n",
      "\tgrad:  2.0 4.0 tensor(-1.0490e-05)\n",
      "\tgrad:  3.0 6.0 tensor(-2.2888e-05)\n",
      "progress: 45 w= tensor([2.0000], requires_grad=True) loss= tensor(1.4552e-11)\n",
      "\tgrad:  1.0 2.0 tensor(-1.9073e-06)\n",
      "\tgrad:  2.0 4.0 tensor(-7.6294e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-1.4305e-05)\n",
      "progress: 46 w= tensor([2.0000], requires_grad=True) loss= tensor(5.6843e-12)\n",
      "\tgrad:  1.0 2.0 tensor(-1.4305e-06)\n",
      "\tgrad:  2.0 4.0 tensor(-5.7220e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-1.1444e-05)\n",
      "progress: 47 w= tensor([2.0000], requires_grad=True) loss= tensor(3.6380e-12)\n",
      "\tgrad:  1.0 2.0 tensor(-1.1921e-06)\n",
      "\tgrad:  2.0 4.0 tensor(-4.7684e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-1.1444e-05)\n",
      "progress: 48 w= tensor([2.0000], requires_grad=True) loss= tensor(3.6380e-12)\n",
      "\tgrad:  1.0 2.0 tensor(-9.5367e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-3.8147e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-8.5831e-06)\n",
      "progress: 49 w= tensor([2.0000], requires_grad=True) loss= tensor(2.0464e-12)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 50 w= tensor([2.0000], requires_grad=True) loss= tensor(9.0949e-13)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 51 w= tensor([2.0000], requires_grad=True) loss= tensor(9.0949e-13)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 52 w= tensor([2.0000], requires_grad=True) loss= tensor(9.0949e-13)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 53 w= tensor([2.0000], requires_grad=True) loss= tensor(9.0949e-13)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 54 w= tensor([2.0000], requires_grad=True) loss= tensor(9.0949e-13)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 55 w= tensor([2.0000], requires_grad=True) loss= tensor(9.0949e-13)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 56 w= tensor([2.0000], requires_grad=True) loss= tensor(9.0949e-13)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 57 w= tensor([2.0000], requires_grad=True) loss= tensor(9.0949e-13)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 58 w= tensor([2.0000], requires_grad=True) loss= tensor(9.0949e-13)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 59 w= tensor([2.0000], requires_grad=True) loss= tensor(9.0949e-13)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 60 w= tensor([2.0000], requires_grad=True) loss= tensor(9.0949e-13)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 61 w= tensor([2.0000], requires_grad=True) loss= tensor(9.0949e-13)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 62 w= tensor([2.0000], requires_grad=True) loss= tensor(9.0949e-13)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 63 w= tensor([2.0000], requires_grad=True) loss= tensor(9.0949e-13)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 64 w= tensor([2.0000], requires_grad=True) loss= tensor(9.0949e-13)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 65 w= tensor([2.0000], requires_grad=True) loss= tensor(9.0949e-13)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 66 w= tensor([2.0000], requires_grad=True) loss= tensor(9.0949e-13)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 67 w= tensor([2.0000], requires_grad=True) loss= tensor(9.0949e-13)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 68 w= tensor([2.0000], requires_grad=True) loss= tensor(9.0949e-13)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 69 w= tensor([2.0000], requires_grad=True) loss= tensor(9.0949e-13)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 70 w= tensor([2.0000], requires_grad=True) loss= tensor(9.0949e-13)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 71 w= tensor([2.0000], requires_grad=True) loss= tensor(9.0949e-13)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 72 w= tensor([2.0000], requires_grad=True) loss= tensor(9.0949e-13)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 73 w= tensor([2.0000], requires_grad=True) loss= tensor(9.0949e-13)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 74 w= tensor([2.0000], requires_grad=True) loss= tensor(9.0949e-13)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 75 w= tensor([2.0000], requires_grad=True) loss= tensor(9.0949e-13)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 76 w= tensor([2.0000], requires_grad=True) loss= tensor(9.0949e-13)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 77 w= tensor([2.0000], requires_grad=True) loss= tensor(9.0949e-13)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 78 w= tensor([2.0000], requires_grad=True) loss= tensor(9.0949e-13)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 79 w= tensor([2.0000], requires_grad=True) loss= tensor(9.0949e-13)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 80 w= tensor([2.0000], requires_grad=True) loss= tensor(9.0949e-13)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 81 w= tensor([2.0000], requires_grad=True) loss= tensor(9.0949e-13)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 82 w= tensor([2.0000], requires_grad=True) loss= tensor(9.0949e-13)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 83 w= tensor([2.0000], requires_grad=True) loss= tensor(9.0949e-13)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 84 w= tensor([2.0000], requires_grad=True) loss= tensor(9.0949e-13)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 85 w= tensor([2.0000], requires_grad=True) loss= tensor(9.0949e-13)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 86 w= tensor([2.0000], requires_grad=True) loss= tensor(9.0949e-13)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 87 w= tensor([2.0000], requires_grad=True) loss= tensor(9.0949e-13)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 88 w= tensor([2.0000], requires_grad=True) loss= tensor(9.0949e-13)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 89 w= tensor([2.0000], requires_grad=True) loss= tensor(9.0949e-13)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 90 w= tensor([2.0000], requires_grad=True) loss= tensor(9.0949e-13)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 91 w= tensor([2.0000], requires_grad=True) loss= tensor(9.0949e-13)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 92 w= tensor([2.0000], requires_grad=True) loss= tensor(9.0949e-13)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 93 w= tensor([2.0000], requires_grad=True) loss= tensor(9.0949e-13)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 94 w= tensor([2.0000], requires_grad=True) loss= tensor(9.0949e-13)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 95 w= tensor([2.0000], requires_grad=True) loss= tensor(9.0949e-13)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 96 w= tensor([2.0000], requires_grad=True) loss= tensor(9.0949e-13)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 97 w= tensor([2.0000], requires_grad=True) loss= tensor(9.0949e-13)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 98 w= tensor([2.0000], requires_grad=True) loss= tensor(9.0949e-13)\n",
      "\tgrad:  1.0 2.0 tensor(-7.1526e-07)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8610e-06)\n",
      "\tgrad:  3.0 6.0 tensor(-5.7220e-06)\n",
      "progress: 99 w= tensor([2.0000], requires_grad=True) loss= tensor(9.0949e-13)\n",
      "predict(after training) 4 tensor(8.0000)\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    for x_val, y_val in zip(x_data, y_data):\n",
    "        l = loss(x_val, y_val) # 손실 정의\n",
    "        l.backward() # backward(): 역전파를 통해 손실 계산 \n",
    "        print(\"\\tgrad: \", x_val, y_val, w.grad.data[0])\n",
    "        w.data = w.data - 0.01 * w.grad.data # 가중치 업데이트 # 계산된 경사도는 w.grad.data에 저장됨\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w.grad.data.zero_()\n",
    "    print(\"progress:\", epoch, \"w=\", w, \"loss=\", l.data[0])\n",
    "\n",
    "# After training\n",
    "print(\"predict(after training)\", 4, forward(4).data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 파이토치의 autograd를 사용한 것과 직접 함수 모델을 제작해서 사용한 것과 결과는 같음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Exercise_1: NumPy를 사용하여 역전파와 계산 그래프를 응용해라"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 가중치, 모델, 손실 함수 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0. weight\n",
    "w = Variable(torch.Tensor([1.0]), requires_grad = True) # Any random value\n",
    "\n",
    "# 1. our forward pass model \n",
    "def forward(x):\n",
    "    return x * w\n",
    "\n",
    "# 2. Loss function\n",
    "def loss(x, y):\n",
    "    y_pred = forward(x)\n",
    "    return (y_pred - y) * (y_pred - y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 경사 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-8.])\n"
     ]
    }
   ],
   "source": [
    "loss(2, 4).backward() # backward(): 역전파를 통해 손실 계산 \n",
    "print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 Exercise_2: 계산 그래프를 이용하여 경사(Gradient)를 계산하라"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 기본 데이터 & 가중치 텐서 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data = [1.0, 2.0, 3.0]\n",
    "y_data = [2.0, 4.0, 6.0]\n",
    "\n",
    "w_1 = Variable(torch.Tensor([1.0]), requires_grad = True) # Any random value\n",
    "w_2 = Variable(torch.Tensor([1.0]), requires_grad = True) # Any random value\n",
    "\n",
    "# 1. 텐서 생성 2. Variable 함수 사용 3. 경사(gradient) 계산 필요 여부 설정 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 모델 & 손실 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predict (before training) 4 tensor(20.)\n"
     ]
    }
   ],
   "source": [
    "# 0. error term 설정\n",
    "\n",
    "b = 0\n",
    "\n",
    "# 1. our forward pass model \n",
    "def forward(x,b):\n",
    "    return (x * x * w_2) + (x * w_1) + b\n",
    "\n",
    "# 2. Loss function\n",
    "def loss(x, b, y):\n",
    "    y_pred = forward(x, b)\n",
    "    return (y_pred - y) * (y_pred - y)\n",
    "\n",
    "# Before training\n",
    "print(\"predict (before training)\", 4, forward(4, 0).data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 훈련: 전파, 역전파, 가중치 업데이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tgrad:  1.0 2.0 tensor(0.) tensor(0.)\n",
      "\tgrad:  2.0 4.0 tensor(8.) tensor(16.)\n",
      "\tgrad:  3.0 6.0 tensor(25.9200) tensor(77.7600)\n",
      "progress: 0 w_1= tensor([0.6608], requires_grad=True) w_2= tensor([0.0624], requires_grad=True) loss= tensor(18.6624)\n",
      "\tgrad:  1.0 2.0 tensor(-2.5536) tensor(-2.5536)\n",
      "\tgrad:  2.0 4.0 tensor(-9.1023) tensor(-18.2047)\n",
      "\tgrad:  3.0 6.0 tensor(-7.4285) tensor(-22.2854)\n",
      "progress: 1 w_1= tensor([0.8516], requires_grad=True) w_2= tensor([0.4928], requires_grad=True) loss= tensor(1.5328)\n",
      "\tgrad:  1.0 2.0 tensor(-1.3110) tensor(-1.3110)\n",
      "\tgrad:  2.0 4.0 tensor(-0.9868) tensor(-1.9736)\n",
      "\tgrad:  3.0 6.0 tensor(8.1301) tensor(24.3903)\n",
      "progress: 2 w_1= tensor([0.7933], requires_grad=True) w_2= tensor([0.2818], requires_grad=True) loss= tensor(1.8361)\n",
      "\tgrad:  1.0 2.0 tensor(-1.8498) tensor(-1.8498)\n",
      "\tgrad:  2.0 4.0 tensor(-4.7010) tensor(-9.4020)\n",
      "\tgrad:  3.0 6.0 tensor(0.7510) tensor(2.2531)\n",
      "progress: 3 w_1= tensor([0.8513], requires_grad=True) w_2= tensor([0.3718], requires_grad=True) loss= tensor(0.0157)\n",
      "\tgrad:  1.0 2.0 tensor(-1.5538) tensor(-1.5538)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8683) tensor(-5.7365)\n",
      "\tgrad:  3.0 6.0 tensor(4.1319) tensor(12.3958)\n",
      "progress: 4 w_1= tensor([0.8542], requires_grad=True) w_2= tensor([0.3207], requires_grad=True) loss= tensor(0.4742)\n",
      "\tgrad:  1.0 2.0 tensor(-1.6501) tensor(-1.6501)\n",
      "\tgrad:  2.0 4.0 tensor(-3.6388) tensor(-7.2776)\n",
      "\tgrad:  3.0 6.0 tensor(2.4674) tensor(7.4023)\n",
      "progress: 5 w_1= tensor([0.8824], requires_grad=True) w_2= tensor([0.3360], requires_grad=True) loss= tensor(0.1691)\n",
      "\tgrad:  1.0 2.0 tensor(-1.5632) tensor(-1.5632)\n",
      "\tgrad:  2.0 4.0 tensor(-3.1899) tensor(-6.3798)\n",
      "\tgrad:  3.0 6.0 tensor(3.1708) tensor(9.5124)\n",
      "progress: 6 w_1= tensor([0.8983], requires_grad=True) w_2= tensor([0.3203], requires_grad=True) loss= tensor(0.2793)\n",
      "\tgrad:  1.0 2.0 tensor(-1.5629) tensor(-1.5629)\n",
      "\tgrad:  2.0 4.0 tensor(-3.3145) tensor(-6.6289)\n",
      "\tgrad:  3.0 6.0 tensor(2.7649) tensor(8.2947)\n",
      "progress: 7 w_1= tensor([0.9194], requires_grad=True) w_2= tensor([0.3192], requires_grad=True) loss= tensor(0.2124)\n",
      "\tgrad:  1.0 2.0 tensor(-1.5227) tensor(-1.5227)\n",
      "\tgrad:  2.0 4.0 tensor(-3.1716) tensor(-6.3431)\n",
      "\tgrad:  3.0 6.0 tensor(2.8806) tensor(8.6419)\n",
      "progress: 8 w_1= tensor([0.9375], requires_grad=True) w_2= tensor([0.3115], requires_grad=True) loss= tensor(0.2305)\n",
      "\tgrad:  1.0 2.0 tensor(-1.5020) tensor(-1.5020)\n",
      "\tgrad:  2.0 4.0 tensor(-3.1556) tensor(-6.3112)\n",
      "\tgrad:  3.0 6.0 tensor(2.7530) tensor(8.2590)\n",
      "progress: 9 w_1= tensor([0.9566], requires_grad=True) w_2= tensor([0.3070], requires_grad=True) loss= tensor(0.2105)\n",
      "\tgrad:  1.0 2.0 tensor(-1.4728) tensor(-1.4728)\n",
      "\tgrad:  2.0 4.0 tensor(-3.0816) tensor(-6.1632)\n",
      "\tgrad:  3.0 6.0 tensor(2.7408) tensor(8.2224)\n",
      "progress: 10 w_1= tensor([0.9747], requires_grad=True) w_2= tensor([0.3012], requires_grad=True) loss= tensor(0.2087)\n",
      "\tgrad:  1.0 2.0 tensor(-1.4483) tensor(-1.4483)\n",
      "\tgrad:  2.0 4.0 tensor(-3.0362) tensor(-6.0724)\n",
      "\tgrad:  3.0 6.0 tensor(2.6757) tensor(8.0272)\n",
      "progress: 11 w_1= tensor([0.9928], requires_grad=True) w_2= tensor([0.2961], requires_grad=True) loss= tensor(0.1989)\n",
      "\tgrad:  1.0 2.0 tensor(-1.4222) tensor(-1.4222)\n",
      "\tgrad:  2.0 4.0 tensor(-2.9788) tensor(-5.9576)\n",
      "\tgrad:  3.0 6.0 tensor(2.6367) tensor(7.9101)\n",
      "progress: 12 w_1= tensor([1.0104], requires_grad=True) w_2= tensor([0.2908], requires_grad=True) loss= tensor(0.1931)\n",
      "\tgrad:  1.0 2.0 tensor(-1.3975) tensor(-1.3975)\n",
      "\tgrad:  2.0 4.0 tensor(-2.9284) tensor(-5.8569)\n",
      "\tgrad:  3.0 6.0 tensor(2.5867) tensor(7.7601)\n",
      "progress: 13 w_1= tensor([1.0278], requires_grad=True) w_2= tensor([0.2857], requires_grad=True) loss= tensor(0.1859)\n",
      "\tgrad:  1.0 2.0 tensor(-1.3729) tensor(-1.3729)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8761) tensor(-5.7522)\n",
      "\tgrad:  3.0 6.0 tensor(2.5430) tensor(7.6291)\n",
      "progress: 14 w_1= tensor([1.0449], requires_grad=True) w_2= tensor([0.2807], requires_grad=True) loss= tensor(0.1796)\n",
      "\tgrad:  1.0 2.0 tensor(-1.3488) tensor(-1.3488)\n",
      "\tgrad:  2.0 4.0 tensor(-2.8260) tensor(-5.6521)\n",
      "\tgrad:  3.0 6.0 tensor(2.4976) tensor(7.4927)\n",
      "progress: 15 w_1= tensor([1.0617], requires_grad=True) w_2= tensor([0.2758], requires_grad=True) loss= tensor(0.1733)\n",
      "\tgrad:  1.0 2.0 tensor(-1.3251) tensor(-1.3251)\n",
      "\tgrad:  2.0 4.0 tensor(-2.7762) tensor(-5.5525)\n",
      "\tgrad:  3.0 6.0 tensor(2.4541) tensor(7.3623)\n",
      "progress: 16 w_1= tensor([1.0781], requires_grad=True) w_2= tensor([0.2709], requires_grad=True) loss= tensor(0.1673)\n",
      "\tgrad:  1.0 2.0 tensor(-1.3019) tensor(-1.3019)\n",
      "\tgrad:  2.0 4.0 tensor(-2.7276) tensor(-5.4552)\n",
      "\tgrad:  3.0 6.0 tensor(2.4108) tensor(7.2325)\n",
      "progress: 17 w_1= tensor([1.0943], requires_grad=True) w_2= tensor([0.2662], requires_grad=True) loss= tensor(0.1614)\n",
      "\tgrad:  1.0 2.0 tensor(-1.2790) tensor(-1.2790)\n",
      "\tgrad:  2.0 4.0 tensor(-2.6796) tensor(-5.3593)\n",
      "\tgrad:  3.0 6.0 tensor(2.3686) tensor(7.1058)\n",
      "progress: 18 w_1= tensor([1.1102], requires_grad=True) w_2= tensor([0.2615], requires_grad=True) loss= tensor(0.1558)\n",
      "\tgrad:  1.0 2.0 tensor(-1.2566) tensor(-1.2566)\n",
      "\tgrad:  2.0 4.0 tensor(-2.6326) tensor(-5.2652)\n",
      "\tgrad:  3.0 6.0 tensor(2.3270) tensor(6.9809)\n",
      "progress: 19 w_1= tensor([1.1258], requires_grad=True) w_2= tensor([0.2569], requires_grad=True) loss= tensor(0.1504)\n",
      "\tgrad:  1.0 2.0 tensor(-1.2345) tensor(-1.2345)\n",
      "\tgrad:  2.0 4.0 tensor(-2.5864) tensor(-5.1728)\n",
      "\tgrad:  3.0 6.0 tensor(2.2861) tensor(6.8584)\n",
      "progress: 20 w_1= tensor([1.1412], requires_grad=True) w_2= tensor([0.2524], requires_grad=True) loss= tensor(0.1452)\n",
      "\tgrad:  1.0 2.0 tensor(-1.2128) tensor(-1.2128)\n",
      "\tgrad:  2.0 4.0 tensor(-2.5410) tensor(-5.0820)\n",
      "\tgrad:  3.0 6.0 tensor(2.2460) tensor(6.7379)\n",
      "progress: 21 w_1= tensor([1.1563], requires_grad=True) w_2= tensor([0.2480], requires_grad=True) loss= tensor(0.1401)\n",
      "\tgrad:  1.0 2.0 tensor(-1.1915) tensor(-1.1915)\n",
      "\tgrad:  2.0 4.0 tensor(-2.4964) tensor(-4.9927)\n",
      "\tgrad:  3.0 6.0 tensor(2.2065) tensor(6.6196)\n",
      "progress: 22 w_1= tensor([1.1711], requires_grad=True) w_2= tensor([0.2436], requires_grad=True) loss= tensor(0.1352)\n",
      "\tgrad:  1.0 2.0 tensor(-1.1706) tensor(-1.1706)\n",
      "\tgrad:  2.0 4.0 tensor(-2.4525) tensor(-4.9051)\n",
      "\tgrad:  3.0 6.0 tensor(2.1678) tensor(6.5034)\n",
      "progress: 23 w_1= tensor([1.1856], requires_grad=True) w_2= tensor([0.2393], requires_grad=True) loss= tensor(0.1305)\n",
      "\tgrad:  1.0 2.0 tensor(-1.1501) tensor(-1.1501)\n",
      "\tgrad:  2.0 4.0 tensor(-2.4095) tensor(-4.8190)\n",
      "\tgrad:  3.0 6.0 tensor(2.1297) tensor(6.3892)\n",
      "progress: 24 w_1= tensor([1.1999], requires_grad=True) w_2= tensor([0.2351], requires_grad=True) loss= tensor(0.1260)\n",
      "\tgrad:  1.0 2.0 tensor(-1.1299) tensor(-1.1299)\n",
      "\tgrad:  2.0 4.0 tensor(-2.3672) tensor(-4.7344)\n",
      "\tgrad:  3.0 6.0 tensor(2.0923) tensor(6.2770)\n",
      "progress: 25 w_1= tensor([1.2140], requires_grad=True) w_2= tensor([0.2310], requires_grad=True) loss= tensor(0.1216)\n",
      "\tgrad:  1.0 2.0 tensor(-1.1100) tensor(-1.1100)\n",
      "\tgrad:  2.0 4.0 tensor(-2.3256) tensor(-4.6512)\n",
      "\tgrad:  3.0 6.0 tensor(2.0556) tensor(6.1668)\n",
      "progress: 26 w_1= tensor([1.2278], requires_grad=True) w_2= tensor([0.2270], requires_grad=True) loss= tensor(0.1174)\n",
      "\tgrad:  1.0 2.0 tensor(-1.0905) tensor(-1.0905)\n",
      "\tgrad:  2.0 4.0 tensor(-2.2848) tensor(-4.5696)\n",
      "\tgrad:  3.0 6.0 tensor(2.0195) tensor(6.0586)\n",
      "progress: 27 w_1= tensor([1.2413], requires_grad=True) w_2= tensor([0.2230], requires_grad=True) loss= tensor(0.1133)\n",
      "\tgrad:  1.0 2.0 tensor(-1.0714) tensor(-1.0714)\n",
      "\tgrad:  2.0 4.0 tensor(-2.2447) tensor(-4.4893)\n",
      "\tgrad:  3.0 6.0 tensor(1.9841) tensor(5.9522)\n",
      "progress: 28 w_1= tensor([1.2547], requires_grad=True) w_2= tensor([0.2191], requires_grad=True) loss= tensor(0.1093)\n",
      "\tgrad:  1.0 2.0 tensor(-1.0526) tensor(-1.0526)\n",
      "\tgrad:  2.0 4.0 tensor(-2.2053) tensor(-4.4105)\n",
      "\tgrad:  3.0 6.0 tensor(1.9492) tensor(5.8477)\n",
      "progress: 29 w_1= tensor([1.2677], requires_grad=True) w_2= tensor([0.2152], requires_grad=True) loss= tensor(0.1055)\n",
      "\tgrad:  1.0 2.0 tensor(-1.0341) tensor(-1.0341)\n",
      "\tgrad:  2.0 4.0 tensor(-2.1665) tensor(-4.3331)\n",
      "\tgrad:  3.0 6.0 tensor(1.9150) tensor(5.7450)\n",
      "progress: 30 w_1= tensor([1.2806], requires_grad=True) w_2= tensor([0.2114], requires_grad=True) loss= tensor(0.1019)\n",
      "\tgrad:  1.0 2.0 tensor(-1.0159) tensor(-1.0159)\n",
      "\tgrad:  2.0 4.0 tensor(-2.1285) tensor(-4.2570)\n",
      "\tgrad:  3.0 6.0 tensor(1.8814) tensor(5.6442)\n",
      "progress: 31 w_1= tensor([1.2932], requires_grad=True) w_2= tensor([0.2077], requires_grad=True) loss= tensor(0.0983)\n",
      "\tgrad:  1.0 2.0 tensor(-0.9981) tensor(-0.9981)\n",
      "\tgrad:  2.0 4.0 tensor(-2.0911) tensor(-4.1823)\n",
      "\tgrad:  3.0 6.0 tensor(1.8484) tensor(5.5451)\n",
      "progress: 32 w_1= tensor([1.3056], requires_grad=True) w_2= tensor([0.2041], requires_grad=True) loss= tensor(0.0949)\n",
      "\tgrad:  1.0 2.0 tensor(-0.9806) tensor(-0.9806)\n",
      "\tgrad:  2.0 4.0 tensor(-2.0544) tensor(-4.1088)\n",
      "\tgrad:  3.0 6.0 tensor(1.8159) tensor(5.4477)\n",
      "progress: 33 w_1= tensor([1.3178], requires_grad=True) w_2= tensor([0.2005], requires_grad=True) loss= tensor(0.0916)\n",
      "\tgrad:  1.0 2.0 tensor(-0.9634) tensor(-0.9634)\n",
      "\tgrad:  2.0 4.0 tensor(-2.0183) tensor(-4.0367)\n",
      "\tgrad:  3.0 6.0 tensor(1.7840) tensor(5.3521)\n",
      "progress: 34 w_1= tensor([1.3298], requires_grad=True) w_2= tensor([0.1970], requires_grad=True) loss= tensor(0.0884)\n",
      "\tgrad:  1.0 2.0 tensor(-0.9465) tensor(-0.9465)\n",
      "\tgrad:  2.0 4.0 tensor(-1.9829) tensor(-3.9658)\n",
      "\tgrad:  3.0 6.0 tensor(1.7527) tensor(5.2581)\n",
      "progress: 35 w_1= tensor([1.3416], requires_grad=True) w_2= tensor([0.1935], requires_grad=True) loss= tensor(0.0853)\n",
      "\tgrad:  1.0 2.0 tensor(-0.9298) tensor(-0.9298)\n",
      "\tgrad:  2.0 4.0 tensor(-1.9481) tensor(-3.8962)\n",
      "\tgrad:  3.0 6.0 tensor(1.7219) tensor(5.1658)\n",
      "progress: 36 w_1= tensor([1.3531], requires_grad=True) w_2= tensor([0.1901], requires_grad=True) loss= tensor(0.0824)\n",
      "\tgrad:  1.0 2.0 tensor(-0.9135) tensor(-0.9135)\n",
      "\tgrad:  2.0 4.0 tensor(-1.9139) tensor(-3.8278)\n",
      "\tgrad:  3.0 6.0 tensor(1.6917) tensor(5.0751)\n",
      "progress: 37 w_1= tensor([1.3645], requires_grad=True) w_2= tensor([0.1868], requires_grad=True) loss= tensor(0.0795)\n",
      "\tgrad:  1.0 2.0 tensor(-0.8975) tensor(-0.8975)\n",
      "\tgrad:  2.0 4.0 tensor(-1.8803) tensor(-3.7606)\n",
      "\tgrad:  3.0 6.0 tensor(1.6620) tensor(4.9860)\n",
      "progress: 38 w_1= tensor([1.3756], requires_grad=True) w_2= tensor([0.1835], requires_grad=True) loss= tensor(0.0767)\n",
      "\tgrad:  1.0 2.0 tensor(-0.8817) tensor(-0.8817)\n",
      "\tgrad:  2.0 4.0 tensor(-1.8473) tensor(-3.6946)\n",
      "\tgrad:  3.0 6.0 tensor(1.6328) tensor(4.8984)\n",
      "progress: 39 w_1= tensor([1.3866], requires_grad=True) w_2= tensor([0.1803], requires_grad=True) loss= tensor(0.0741)\n",
      "\tgrad:  1.0 2.0 tensor(-0.8662) tensor(-0.8662)\n",
      "\tgrad:  2.0 4.0 tensor(-1.8148) tensor(-3.6297)\n",
      "\tgrad:  3.0 6.0 tensor(1.6041) tensor(4.8124)\n",
      "progress: 40 w_1= tensor([1.3974], requires_grad=True) w_2= tensor([0.1771], requires_grad=True) loss= tensor(0.0715)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tgrad:  1.0 2.0 tensor(-0.8510) tensor(-0.8510)\n",
      "\tgrad:  2.0 4.0 tensor(-1.7830) tensor(-3.5660)\n",
      "\tgrad:  3.0 6.0 tensor(1.5760) tensor(4.7279)\n",
      "progress: 41 w_1= tensor([1.4080], requires_grad=True) w_2= tensor([0.1740], requires_grad=True) loss= tensor(0.0690)\n",
      "\tgrad:  1.0 2.0 tensor(-0.8361) tensor(-0.8361)\n",
      "\tgrad:  2.0 4.0 tensor(-1.7517) tensor(-3.5034)\n",
      "\tgrad:  3.0 6.0 tensor(1.5483) tensor(4.6449)\n",
      "progress: 42 w_1= tensor([1.4184], requires_grad=True) w_2= tensor([0.1709], requires_grad=True) loss= tensor(0.0666)\n",
      "\tgrad:  1.0 2.0 tensor(-0.8214) tensor(-0.8214)\n",
      "\tgrad:  2.0 4.0 tensor(-1.7209) tensor(-3.4419)\n",
      "\tgrad:  3.0 6.0 tensor(1.5211) tensor(4.5634)\n",
      "progress: 43 w_1= tensor([1.4286], requires_grad=True) w_2= tensor([0.1679], requires_grad=True) loss= tensor(0.0643)\n",
      "\tgrad:  1.0 2.0 tensor(-0.8070) tensor(-0.8070)\n",
      "\tgrad:  2.0 4.0 tensor(-1.6907) tensor(-3.3814)\n",
      "\tgrad:  3.0 6.0 tensor(1.4944) tensor(4.4833)\n",
      "progress: 44 w_1= tensor([1.4386], requires_grad=True) w_2= tensor([0.1650], requires_grad=True) loss= tensor(0.0620)\n",
      "\tgrad:  1.0 2.0 tensor(-0.7928) tensor(-0.7928)\n",
      "\tgrad:  2.0 4.0 tensor(-1.6610) tensor(-3.3221)\n",
      "\tgrad:  3.0 6.0 tensor(1.4682) tensor(4.4046)\n",
      "progress: 45 w_1= tensor([1.4485], requires_grad=True) w_2= tensor([0.1621], requires_grad=True) loss= tensor(0.0599)\n",
      "\tgrad:  1.0 2.0 tensor(-0.7789) tensor(-0.7789)\n",
      "\tgrad:  2.0 4.0 tensor(-1.6319) tensor(-3.2637)\n",
      "\tgrad:  3.0 6.0 tensor(1.4424) tensor(4.3272)\n",
      "progress: 46 w_1= tensor([1.4581], requires_grad=True) w_2= tensor([0.1593], requires_grad=True) loss= tensor(0.0578)\n",
      "\tgrad:  1.0 2.0 tensor(-0.7652) tensor(-0.7652)\n",
      "\tgrad:  2.0 4.0 tensor(-1.6032) tensor(-3.2064)\n",
      "\tgrad:  3.0 6.0 tensor(1.4171) tensor(4.2513)\n",
      "progress: 47 w_1= tensor([1.4677], requires_grad=True) w_2= tensor([0.1565], requires_grad=True) loss= tensor(0.0558)\n",
      "\tgrad:  1.0 2.0 tensor(-0.7518) tensor(-0.7518)\n",
      "\tgrad:  2.0 4.0 tensor(-1.5751) tensor(-3.1501)\n",
      "\tgrad:  3.0 6.0 tensor(1.3922) tensor(4.1766)\n",
      "progress: 48 w_1= tensor([1.4770], requires_grad=True) w_2= tensor([0.1537], requires_grad=True) loss= tensor(0.0538)\n",
      "\tgrad:  1.0 2.0 tensor(-0.7386) tensor(-0.7386)\n",
      "\tgrad:  2.0 4.0 tensor(-1.5474) tensor(-3.0948)\n",
      "\tgrad:  3.0 6.0 tensor(1.3678) tensor(4.1033)\n",
      "progress: 49 w_1= tensor([1.4862], requires_grad=True) w_2= tensor([0.1510], requires_grad=True) loss= tensor(0.0520)\n",
      "\tgrad:  1.0 2.0 tensor(-0.7256) tensor(-0.7256)\n",
      "\tgrad:  2.0 4.0 tensor(-1.5202) tensor(-3.0405)\n",
      "\tgrad:  3.0 6.0 tensor(1.3437) tensor(4.0312)\n",
      "progress: 50 w_1= tensor([1.4952], requires_grad=True) w_2= tensor([0.1484], requires_grad=True) loss= tensor(0.0502)\n",
      "\tgrad:  1.0 2.0 tensor(-0.7129) tensor(-0.7129)\n",
      "\tgrad:  2.0 4.0 tensor(-1.4936) tensor(-2.9871)\n",
      "\tgrad:  3.0 6.0 tensor(1.3202) tensor(3.9605)\n",
      "progress: 51 w_1= tensor([1.5041], requires_grad=True) w_2= tensor([0.1458], requires_grad=True) loss= tensor(0.0484)\n",
      "\tgrad:  1.0 2.0 tensor(-0.7004) tensor(-0.7004)\n",
      "\tgrad:  2.0 4.0 tensor(-1.4673) tensor(-2.9347)\n",
      "\tgrad:  3.0 6.0 tensor(1.2970) tensor(3.8909)\n",
      "progress: 52 w_1= tensor([1.5128], requires_grad=True) w_2= tensor([0.1432], requires_grad=True) loss= tensor(0.0467)\n",
      "\tgrad:  1.0 2.0 tensor(-0.6881) tensor(-0.6881)\n",
      "\tgrad:  2.0 4.0 tensor(-1.4416) tensor(-2.8831)\n",
      "\tgrad:  3.0 6.0 tensor(1.2742) tensor(3.8226)\n",
      "progress: 53 w_1= tensor([1.5213], requires_grad=True) w_2= tensor([0.1407], requires_grad=True) loss= tensor(0.0451)\n",
      "\tgrad:  1.0 2.0 tensor(-0.6760) tensor(-0.6760)\n",
      "\tgrad:  2.0 4.0 tensor(-1.4163) tensor(-2.8325)\n",
      "\tgrad:  3.0 6.0 tensor(1.2518) tensor(3.7555)\n",
      "progress: 54 w_1= tensor([1.5297], requires_grad=True) w_2= tensor([0.1382], requires_grad=True) loss= tensor(0.0435)\n",
      "\tgrad:  1.0 2.0 tensor(-0.6641) tensor(-0.6641)\n",
      "\tgrad:  2.0 4.0 tensor(-1.3914) tensor(-2.7828)\n",
      "\tgrad:  3.0 6.0 tensor(1.2299) tensor(3.6896)\n",
      "progress: 55 w_1= tensor([1.5380], requires_grad=True) w_2= tensor([0.1358], requires_grad=True) loss= tensor(0.0420)\n",
      "\tgrad:  1.0 2.0 tensor(-0.6525) tensor(-0.6525)\n",
      "\tgrad:  2.0 4.0 tensor(-1.3670) tensor(-2.7339)\n",
      "\tgrad:  3.0 6.0 tensor(1.2083) tensor(3.6248)\n",
      "progress: 56 w_1= tensor([1.5461], requires_grad=True) w_2= tensor([0.1334], requires_grad=True) loss= tensor(0.0406)\n",
      "\tgrad:  1.0 2.0 tensor(-0.6410) tensor(-0.6410)\n",
      "\tgrad:  2.0 4.0 tensor(-1.3430) tensor(-2.6859)\n",
      "\tgrad:  3.0 6.0 tensor(1.1870) tensor(3.5611)\n",
      "progress: 57 w_1= tensor([1.5541], requires_grad=True) w_2= tensor([0.1311], requires_grad=True) loss= tensor(0.0391)\n",
      "\tgrad:  1.0 2.0 tensor(-0.6297) tensor(-0.6297)\n",
      "\tgrad:  2.0 4.0 tensor(-1.3194) tensor(-2.6388)\n",
      "\tgrad:  3.0 6.0 tensor(1.1662) tensor(3.4986)\n",
      "progress: 58 w_1= tensor([1.5619], requires_grad=True) w_2= tensor([0.1288], requires_grad=True) loss= tensor(0.0378)\n",
      "\tgrad:  1.0 2.0 tensor(-0.6187) tensor(-0.6187)\n",
      "\tgrad:  2.0 4.0 tensor(-1.2962) tensor(-2.5924)\n",
      "\tgrad:  3.0 6.0 tensor(1.1457) tensor(3.4372)\n",
      "progress: 59 w_1= tensor([1.5696], requires_grad=True) w_2= tensor([0.1265], requires_grad=True) loss= tensor(0.0365)\n",
      "\tgrad:  1.0 2.0 tensor(-0.6078) tensor(-0.6078)\n",
      "\tgrad:  2.0 4.0 tensor(-1.2735) tensor(-2.5469)\n",
      "\tgrad:  3.0 6.0 tensor(1.1256) tensor(3.3768)\n",
      "progress: 60 w_1= tensor([1.5771], requires_grad=True) w_2= tensor([0.1243], requires_grad=True) loss= tensor(0.0352)\n",
      "\tgrad:  1.0 2.0 tensor(-0.5972) tensor(-0.5972)\n",
      "\tgrad:  2.0 4.0 tensor(-1.2511) tensor(-2.5022)\n",
      "\tgrad:  3.0 6.0 tensor(1.1059) tensor(3.3176)\n",
      "progress: 61 w_1= tensor([1.5846], requires_grad=True) w_2= tensor([0.1221], requires_grad=True) loss= tensor(0.0340)\n",
      "\tgrad:  1.0 2.0 tensor(-0.5867) tensor(-0.5867)\n",
      "\tgrad:  2.0 4.0 tensor(-1.2291) tensor(-2.4583)\n",
      "\tgrad:  3.0 6.0 tensor(1.0864) tensor(3.2593)\n",
      "progress: 62 w_1= tensor([1.5919], requires_grad=True) w_2= tensor([0.1200], requires_grad=True) loss= tensor(0.0328)\n",
      "\tgrad:  1.0 2.0 tensor(-0.5764) tensor(-0.5764)\n",
      "\tgrad:  2.0 4.0 tensor(-1.2076) tensor(-2.4151)\n",
      "\tgrad:  3.0 6.0 tensor(1.0674) tensor(3.2021)\n",
      "progress: 63 w_1= tensor([1.5990], requires_grad=True) w_2= tensor([0.1178], requires_grad=True) loss= tensor(0.0316)\n",
      "\tgrad:  1.0 2.0 tensor(-0.5663) tensor(-0.5663)\n",
      "\tgrad:  2.0 4.0 tensor(-1.1864) tensor(-2.3727)\n",
      "\tgrad:  3.0 6.0 tensor(1.0486) tensor(3.1459)\n",
      "progress: 64 w_1= tensor([1.6061], requires_grad=True) w_2= tensor([0.1158], requires_grad=True) loss= tensor(0.0305)\n",
      "\tgrad:  1.0 2.0 tensor(-0.5563) tensor(-0.5563)\n",
      "\tgrad:  2.0 4.0 tensor(-1.1655) tensor(-2.3311)\n",
      "\tgrad:  3.0 6.0 tensor(1.0302) tensor(3.0906)\n",
      "progress: 65 w_1= tensor([1.6130], requires_grad=True) w_2= tensor([0.1137], requires_grad=True) loss= tensor(0.0295)\n",
      "\tgrad:  1.0 2.0 tensor(-0.5465) tensor(-0.5465)\n",
      "\tgrad:  2.0 4.0 tensor(-1.1451) tensor(-2.2901)\n",
      "\tgrad:  3.0 6.0 tensor(1.0121) tensor(3.0364)\n",
      "progress: 66 w_1= tensor([1.6198], requires_grad=True) w_2= tensor([0.1117], requires_grad=True) loss= tensor(0.0285)\n",
      "\tgrad:  1.0 2.0 tensor(-0.5369) tensor(-0.5369)\n",
      "\tgrad:  2.0 4.0 tensor(-1.1250) tensor(-2.2499)\n",
      "\tgrad:  3.0 6.0 tensor(0.9944) tensor(2.9831)\n",
      "progress: 67 w_1= tensor([1.6265], requires_grad=True) w_2= tensor([0.1098], requires_grad=True) loss= tensor(0.0275)\n",
      "\tgrad:  1.0 2.0 tensor(-0.5275) tensor(-0.5275)\n",
      "\tgrad:  2.0 4.0 tensor(-1.1052) tensor(-2.2104)\n",
      "\tgrad:  3.0 6.0 tensor(0.9769) tensor(2.9307)\n",
      "progress: 68 w_1= tensor([1.6330], requires_grad=True) w_2= tensor([0.1079], requires_grad=True) loss= tensor(0.0265)\n",
      "\tgrad:  1.0 2.0 tensor(-0.5183) tensor(-0.5183)\n",
      "\tgrad:  2.0 4.0 tensor(-1.0858) tensor(-2.1716)\n",
      "\tgrad:  3.0 6.0 tensor(0.9597) tensor(2.8792)\n",
      "progress: 69 w_1= tensor([1.6395], requires_grad=True) w_2= tensor([0.1060], requires_grad=True) loss= tensor(0.0256)\n",
      "\tgrad:  1.0 2.0 tensor(-0.5092) tensor(-0.5092)\n",
      "\tgrad:  2.0 4.0 tensor(-1.0667) tensor(-2.1335)\n",
      "\tgrad:  3.0 6.0 tensor(0.9429) tensor(2.8287)\n",
      "progress: 70 w_1= tensor([1.6458], requires_grad=True) w_2= tensor([0.1041], requires_grad=True) loss= tensor(0.0247)\n",
      "\tgrad:  1.0 2.0 tensor(-0.5002) tensor(-0.5002)\n",
      "\tgrad:  2.0 4.0 tensor(-1.0480) tensor(-2.0960)\n",
      "\tgrad:  3.0 6.0 tensor(0.9263) tensor(2.7790)\n",
      "progress: 71 w_1= tensor([1.6520], requires_grad=True) w_2= tensor([0.1023], requires_grad=True) loss= tensor(0.0238)\n",
      "\tgrad:  1.0 2.0 tensor(-0.4914) tensor(-0.4914)\n",
      "\tgrad:  2.0 4.0 tensor(-1.0296) tensor(-2.0592)\n",
      "\tgrad:  3.0 6.0 tensor(0.9101) tensor(2.7302)\n",
      "progress: 72 w_1= tensor([1.6581], requires_grad=True) w_2= tensor([0.1005], requires_grad=True) loss= tensor(0.0230)\n",
      "\tgrad:  1.0 2.0 tensor(-0.4828) tensor(-0.4828)\n",
      "\tgrad:  2.0 4.0 tensor(-1.0115) tensor(-2.0231)\n",
      "\tgrad:  3.0 6.0 tensor(0.8941) tensor(2.6823)\n",
      "progress: 73 w_1= tensor([1.6641], requires_grad=True) w_2= tensor([0.0987], requires_grad=True) loss= tensor(0.0222)\n",
      "\tgrad:  1.0 2.0 tensor(-0.4743) tensor(-0.4743)\n",
      "\tgrad:  2.0 4.0 tensor(-0.9938) tensor(-1.9876)\n",
      "\tgrad:  3.0 6.0 tensor(0.8784) tensor(2.6352)\n",
      "progress: 74 w_1= tensor([1.6700], requires_grad=True) w_2= tensor([0.0970], requires_grad=True) loss= tensor(0.0214)\n",
      "\tgrad:  1.0 2.0 tensor(-0.4660) tensor(-0.4660)\n",
      "\tgrad:  2.0 4.0 tensor(-0.9763) tensor(-1.9527)\n",
      "\tgrad:  3.0 6.0 tensor(0.8630) tensor(2.5889)\n",
      "progress: 75 w_1= tensor([1.6758], requires_grad=True) w_2= tensor([0.0953], requires_grad=True) loss= tensor(0.0207)\n",
      "\tgrad:  1.0 2.0 tensor(-0.4578) tensor(-0.4578)\n",
      "\tgrad:  2.0 4.0 tensor(-0.9592) tensor(-1.9184)\n",
      "\tgrad:  3.0 6.0 tensor(0.8478) tensor(2.5435)\n",
      "progress: 76 w_1= tensor([1.6815], requires_grad=True) w_2= tensor([0.0936], requires_grad=True) loss= tensor(0.0200)\n",
      "\tgrad:  1.0 2.0 tensor(-0.4498) tensor(-0.4498)\n",
      "\tgrad:  2.0 4.0 tensor(-0.9423) tensor(-1.8847)\n",
      "\tgrad:  3.0 6.0 tensor(0.8329) tensor(2.4988)\n",
      "progress: 77 w_1= tensor([1.6871], requires_grad=True) w_2= tensor([0.0920], requires_grad=True) loss= tensor(0.0193)\n",
      "\tgrad:  1.0 2.0 tensor(-0.4419) tensor(-0.4419)\n",
      "\tgrad:  2.0 4.0 tensor(-0.9258) tensor(-1.8516)\n",
      "\tgrad:  3.0 6.0 tensor(0.8183) tensor(2.4549)\n",
      "progress: 78 w_1= tensor([1.6926], requires_grad=True) w_2= tensor([0.0903], requires_grad=True) loss= tensor(0.0186)\n",
      "\tgrad:  1.0 2.0 tensor(-0.4341) tensor(-0.4341)\n",
      "\tgrad:  2.0 4.0 tensor(-0.9095) tensor(-1.8191)\n",
      "\tgrad:  3.0 6.0 tensor(0.8040) tensor(2.4119)\n",
      "progress: 79 w_1= tensor([1.6980], requires_grad=True) w_2= tensor([0.0888], requires_grad=True) loss= tensor(0.0180)\n",
      "\tgrad:  1.0 2.0 tensor(-0.4265) tensor(-0.4265)\n",
      "\tgrad:  2.0 4.0 tensor(-0.8936) tensor(-1.7872)\n",
      "\tgrad:  3.0 6.0 tensor(0.7898) tensor(2.3695)\n",
      "progress: 80 w_1= tensor([1.7033], requires_grad=True) w_2= tensor([0.0872], requires_grad=True) loss= tensor(0.0173)\n",
      "\tgrad:  1.0 2.0 tensor(-0.4190) tensor(-0.4190)\n",
      "\tgrad:  2.0 4.0 tensor(-0.8779) tensor(-1.7558)\n",
      "\tgrad:  3.0 6.0 tensor(0.7760) tensor(2.3279)\n",
      "progress: 81 w_1= tensor([1.7085], requires_grad=True) w_2= tensor([0.0857], requires_grad=True) loss= tensor(0.0167)\n",
      "\tgrad:  1.0 2.0 tensor(-0.4117) tensor(-0.4117)\n",
      "\tgrad:  2.0 4.0 tensor(-0.8625) tensor(-1.7250)\n",
      "\tgrad:  3.0 6.0 tensor(0.7623) tensor(2.2870)\n",
      "progress: 82 w_1= tensor([1.7136], requires_grad=True) w_2= tensor([0.0842], requires_grad=True) loss= tensor(0.0161)\n",
      "\tgrad:  1.0 2.0 tensor(-0.4044) tensor(-0.4044)\n",
      "\tgrad:  2.0 4.0 tensor(-0.8473) tensor(-1.6947)\n",
      "\tgrad:  3.0 6.0 tensor(0.7490) tensor(2.2469)\n",
      "progress: 83 w_1= tensor([1.7186], requires_grad=True) w_2= tensor([0.0827], requires_grad=True) loss= tensor(0.0156)\n",
      "\tgrad:  1.0 2.0 tensor(-0.3973) tensor(-0.3973)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tgrad:  2.0 4.0 tensor(-0.8325) tensor(-1.6649)\n",
      "\tgrad:  3.0 6.0 tensor(0.7358) tensor(2.2074)\n",
      "progress: 84 w_1= tensor([1.7236], requires_grad=True) w_2= tensor([0.0812], requires_grad=True) loss= tensor(0.0150)\n",
      "\tgrad:  1.0 2.0 tensor(-0.3904) tensor(-0.3904)\n",
      "\tgrad:  2.0 4.0 tensor(-0.8178) tensor(-1.6357)\n",
      "\tgrad:  3.0 6.0 tensor(0.7229) tensor(2.1687)\n",
      "progress: 85 w_1= tensor([1.7284], requires_grad=True) w_2= tensor([0.0798], requires_grad=True) loss= tensor(0.0145)\n",
      "\tgrad:  1.0 2.0 tensor(-0.3835) tensor(-0.3835)\n",
      "\tgrad:  2.0 4.0 tensor(-0.8035) tensor(-1.6070)\n",
      "\tgrad:  3.0 6.0 tensor(0.7102) tensor(2.1306)\n",
      "progress: 86 w_1= tensor([1.7332], requires_grad=True) w_2= tensor([0.0784], requires_grad=True) loss= tensor(0.0140)\n",
      "\tgrad:  1.0 2.0 tensor(-0.3768) tensor(-0.3768)\n",
      "\tgrad:  2.0 4.0 tensor(-0.7894) tensor(-1.5788)\n",
      "\tgrad:  3.0 6.0 tensor(0.6977) tensor(2.0932)\n",
      "progress: 87 w_1= tensor([1.7379], requires_grad=True) w_2= tensor([0.0770], requires_grad=True) loss= tensor(0.0135)\n",
      "\tgrad:  1.0 2.0 tensor(-0.3702) tensor(-0.3702)\n",
      "\tgrad:  2.0 4.0 tensor(-0.7755) tensor(-1.5510)\n",
      "\tgrad:  3.0 6.0 tensor(0.6855) tensor(2.0564)\n",
      "progress: 88 w_1= tensor([1.7425], requires_grad=True) w_2= tensor([0.0757], requires_grad=True) loss= tensor(0.0131)\n",
      "\tgrad:  1.0 2.0 tensor(-0.3637) tensor(-0.3637)\n",
      "\tgrad:  2.0 4.0 tensor(-0.7619) tensor(-1.5238)\n",
      "\tgrad:  3.0 6.0 tensor(0.6734) tensor(2.0203)\n",
      "progress: 89 w_1= tensor([1.7470], requires_grad=True) w_2= tensor([0.0744], requires_grad=True) loss= tensor(0.0126)\n",
      "\tgrad:  1.0 2.0 tensor(-0.3573) tensor(-0.3573)\n",
      "\tgrad:  2.0 4.0 tensor(-0.7485) tensor(-1.4970)\n",
      "\tgrad:  3.0 6.0 tensor(0.6616) tensor(1.9849)\n",
      "progress: 90 w_1= tensor([1.7515], requires_grad=True) w_2= tensor([0.0730], requires_grad=True) loss= tensor(0.0122)\n",
      "\tgrad:  1.0 2.0 tensor(-0.3510) tensor(-0.3510)\n",
      "\tgrad:  2.0 4.0 tensor(-0.7354) tensor(-1.4708)\n",
      "\tgrad:  3.0 6.0 tensor(0.6500) tensor(1.9500)\n",
      "progress: 91 w_1= tensor([1.7558], requires_grad=True) w_2= tensor([0.0718], requires_grad=True) loss= tensor(0.0117)\n",
      "\tgrad:  1.0 2.0 tensor(-0.3448) tensor(-0.3448)\n",
      "\tgrad:  2.0 4.0 tensor(-0.7225) tensor(-1.4449)\n",
      "\tgrad:  3.0 6.0 tensor(0.6386) tensor(1.9158)\n",
      "progress: 92 w_1= tensor([1.7601], requires_grad=True) w_2= tensor([0.0705], requires_grad=True) loss= tensor(0.0113)\n",
      "\tgrad:  1.0 2.0 tensor(-0.3388) tensor(-0.3388)\n",
      "\tgrad:  2.0 4.0 tensor(-0.7098) tensor(-1.4196)\n",
      "\tgrad:  3.0 6.0 tensor(0.6274) tensor(1.8821)\n",
      "progress: 93 w_1= tensor([1.7643], requires_grad=True) w_2= tensor([0.0693], requires_grad=True) loss= tensor(0.0109)\n",
      "\tgrad:  1.0 2.0 tensor(-0.3328) tensor(-0.3328)\n",
      "\tgrad:  2.0 4.0 tensor(-0.6973) tensor(-1.3946)\n",
      "\tgrad:  3.0 6.0 tensor(0.6164) tensor(1.8491)\n",
      "progress: 94 w_1= tensor([1.7685], requires_grad=True) w_2= tensor([0.0681], requires_grad=True) loss= tensor(0.0106)\n",
      "\tgrad:  1.0 2.0 tensor(-0.3270) tensor(-0.3270)\n",
      "\tgrad:  2.0 4.0 tensor(-0.6851) tensor(-1.3702)\n",
      "\tgrad:  3.0 6.0 tensor(0.6055) tensor(1.8166)\n",
      "progress: 95 w_1= tensor([1.7725], requires_grad=True) w_2= tensor([0.0669], requires_grad=True) loss= tensor(0.0102)\n",
      "\tgrad:  1.0 2.0 tensor(-0.3213) tensor(-0.3213)\n",
      "\tgrad:  2.0 4.0 tensor(-0.6731) tensor(-1.3461)\n",
      "\tgrad:  3.0 6.0 tensor(0.5949) tensor(1.7847)\n",
      "progress: 96 w_1= tensor([1.7765], requires_grad=True) w_2= tensor([0.0657], requires_grad=True) loss= tensor(0.0098)\n",
      "\tgrad:  1.0 2.0 tensor(-0.3156) tensor(-0.3156)\n",
      "\tgrad:  2.0 4.0 tensor(-0.6612) tensor(-1.3225)\n",
      "\tgrad:  3.0 6.0 tensor(0.5845) tensor(1.7534)\n",
      "progress: 97 w_1= tensor([1.7804], requires_grad=True) w_2= tensor([0.0645], requires_grad=True) loss= tensor(0.0095)\n",
      "\tgrad:  1.0 2.0 tensor(-0.3101) tensor(-0.3101)\n",
      "\tgrad:  2.0 4.0 tensor(-0.6496) tensor(-1.2993)\n",
      "\tgrad:  3.0 6.0 tensor(0.5742) tensor(1.7226)\n",
      "progress: 98 w_1= tensor([1.7843], requires_grad=True) w_2= tensor([0.0634], requires_grad=True) loss= tensor(0.0092)\n",
      "\tgrad:  1.0 2.0 tensor(-0.3046) tensor(-0.3046)\n",
      "\tgrad:  2.0 4.0 tensor(-0.6382) tensor(-1.2764)\n",
      "\tgrad:  3.0 6.0 tensor(0.5641) tensor(1.6924)\n",
      "progress: 99 w_1= tensor([1.7881], requires_grad=True) w_2= tensor([0.0623], requires_grad=True) loss= tensor(0.0088)\n",
      "predict(after training) 4 tensor(8.1488)\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    for x_val, y_val in zip(x_data, y_data):\n",
    "        l = loss(x_val, b, y_val) # 손실 정의\n",
    "        l.backward() # backward(): 역전파를 통해 손실 계산 \n",
    "        print(\"\\tgrad: \", x_val, y_val, w_1.grad.data[0], w_2.grad.data[0])\n",
    "        w_1.data = w_1.data - 0.01 * w_1.grad.data # 가중치 업데이트 # 계산된 경사도는 w_1.grad.data에 저장됨\n",
    "        w_2.data = w_2.data - 0.01 * w_2.grad.data # 가중치 업데이트 # 계산된 경사도는 w_2.grad.data에 저장됨\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w_1.grad.data.zero_()\n",
    "        w_2.grad.data.zero_()\n",
    "    print(\"progress:\", epoch, \"w_1=\", w_1, \"w_2=\", w_2, \"loss=\", l.data[0])\n",
    "\n",
    "# After training\n",
    "print(\"predict(after training)\", 4, forward(4, 0).data[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37_64",
   "language": "python",
   "name": "py37_64"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
